{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "from os.path import normpath as fn # Fix Linux/Windows path issue\n",
    "import sys\n",
    "sys.path.append(\"nn\") # add the nn module into system path\n",
    "\n",
    "from nn.container import Placeholder, Sequential\n",
    "from nn.solver import Adam\n",
    "from nn.loss import SmaxCELoss, accuracy\n",
    "from nn.graph import Graph, Seesion\n",
    "from nn import layer\n",
    "\n",
    "np.random.seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load data\n",
    "data = np.load(fn('data/mnist_26k.npz'))\n",
    "train_im = np.float32(data['im_train'])/255.-0.5\n",
    "train_im = np.reshape(train_im,[-1,28,28,1])\n",
    "train_lb = data['lbl_train']\n",
    "\n",
    "val_im = np.float32(data['im_val'])/255.-0.5\n",
    "val_im = np.reshape(val_im,[-1,28,28,1])\n",
    "val_lb = data['lbl_val']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "C1 = 16\n",
    "C2 = 32\n",
    "\n",
    "###################################\n",
    "# build static computational graph\n",
    "###################################\n",
    "graph = Graph()\n",
    "graph.as_default()\n",
    "\n",
    "# placeholder for input\n",
    "inp = Placeholder()\n",
    "lab = Placeholder()\n",
    "\n",
    "model = Sequential([layer.Conv2d(1, C1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                           layer.BatchNorm2d(C1),\n",
    "                           layer.RELU(),\n",
    "                           layer.Maxpool2d(),\n",
    "                           layer.Conv2d(C1, C2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                           layer.BatchNorm2d(C2),\n",
    "                           layer.RELU(),\n",
    "                           layer.Maxpool2d(),\n",
    "                           layer.Dropout(0.3),\n",
    "                           layer.Flatten(),\n",
    "                           layer.Linear(C2*49 , 10)\n",
    "                          ])\n",
    "print(model)\n",
    "\n",
    "# build computational graph for model\n",
    "y = model(inp)\n",
    "\n",
    "# loss function: softmax + crossentropy\n",
    "criterion = SmaxCELoss()\n",
    "\n",
    "# Cross Entropy of Soft-max\n",
    "loss = criterion(y, lab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(main) Sequential(\n",
      "  (0) Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False)\n",
      "  (1) BatchNorm2d(num_features=16, eps=1e-05, momentum=0.1)\n",
      "  (2) RELU()\n",
      "  (3) Maxpool2d(kernel_size=2, stride=2)\n",
      "  (4) Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
      "  (5) BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1)\n",
      "  (6) RELU()\n",
      "  (7) Maxpool2d(kernel_size=2, stride=2)\n",
      "  (8) Dropout(p=0.3)\n",
      "  (9) Flatten()\n",
      "  (10) Linear(in_features=1568, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "lr=1e-3\n",
    "NUM_EPOCH=10\n",
    "DISPITER=100\n",
    "BSZ=64\n",
    "\n",
    "optimizer = Adam(graph.variables, lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Training loop\n",
    "batches = range(0,len(train_lb)-BSZ+1,BSZ)\n",
    "niter, avg_loss, avg_acc=0, 0., 0.\n",
    "\n",
    "sess = Seesion()\n",
    "for ep in range(NUM_EPOCH+1):\n",
    "    sess.eval()\n",
    "    # As we train, let's keep track of val accuracy\n",
    "    vacc, vloss, viter= 0., 0., 0\n",
    "    for b in range(0,len(val_lb)-BSZ+1,BSZ):\n",
    "        sess.run(loss, {inp: val_im[b:b+BSZ,...], lab:val_lb[b:b+BSZ]})\n",
    "        viter += 1\n",
    "        vacc += accuracy(y.top, val_lb[b:b+BSZ])\n",
    "        vloss += loss.top\n",
    "    vloss, vacc = vloss / viter, vacc / viter * 100\n",
    "    print(\"%09d: #### %d Epochs: Val Loss = %.3e, Accuracy = %.2f%%\" % (niter,ep,vloss,vacc))\n",
    "    if ep == NUM_EPOCH:\n",
    "        break\n",
    "    \n",
    "    # Shuffle Training Set\n",
    "    idx = np.random.permutation(len(train_lb))\n",
    "    sess.train()\n",
    "    # Train one epoch\n",
    "    for b in batches:\n",
    "        # Load a batch\n",
    "        sess.run(loss, {inp: train_im[idx[b:b+BSZ],...], lab: train_lb[idx[b:b+BSZ]]})\n",
    "        avg_loss += loss.top \n",
    "        avg_acc += accuracy(y.top, train_lb[idx[b:b+BSZ]])\n",
    "        niter += 1\n",
    "        if niter % DISPITER == 0:\n",
    "            avg_loss = avg_loss / DISPITER\n",
    "            avg_acc = avg_acc / DISPITER * 100\n",
    "            print(\"%09d: Training Loss = %.3e, Accuracy = %.2f%%\" % (niter,avg_loss,avg_acc))\n",
    "            avg_loss, avg_acc = 0., 0.\n",
    "        \n",
    "        optimizer.step(niter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000000000: #### 0 Epochs: Val Loss = 2.330e+00, Accuracy = 7.40%\n",
      "000000100: Training Loss = 3.384e+00, Accuracy = 12.45%\n",
      "000000200: Training Loss = 2.906e+00, Accuracy = 17.30%\n",
      "000000300: Training Loss = 2.440e+00, Accuracy = 25.48%\n",
      "000000390: #### 1 Epochs: Val Loss = 1.033e+00, Accuracy = 77.08%\n",
      "000000400: Training Loss = 1.995e+00, Accuracy = 36.38%\n",
      "000000500: Training Loss = 1.723e+00, Accuracy = 43.30%\n",
      "000000600: Training Loss = 1.460e+00, Accuracy = 50.91%\n",
      "000000700: Training Loss = 1.305e+00, Accuracy = 56.25%\n",
      "000000780: #### 2 Epochs: Val Loss = 5.396e-01, Accuracy = 88.12%\n",
      "000000800: Training Loss = 1.129e+00, Accuracy = 62.66%\n",
      "000000900: Training Loss = 9.904e-01, Accuracy = 67.05%\n",
      "000001000: Training Loss = 9.651e-01, Accuracy = 67.97%\n",
      "000001100: Training Loss = 8.953e-01, Accuracy = 70.22%\n",
      "000001170: #### 3 Epochs: Val Loss = 3.900e-01, Accuracy = 91.04%\n",
      "000001200: Training Loss = 8.346e-01, Accuracy = 72.55%\n",
      "000001300: Training Loss = 7.782e-01, Accuracy = 74.02%\n",
      "000001400: Training Loss = 7.392e-01, Accuracy = 75.94%\n",
      "000001500: Training Loss = 6.850e-01, Accuracy = 78.06%\n",
      "000001560: #### 4 Epochs: Val Loss = 3.165e-01, Accuracy = 91.67%\n",
      "000001600: Training Loss = 6.814e-01, Accuracy = 77.70%\n",
      "000001700: Training Loss = 6.490e-01, Accuracy = 78.20%\n",
      "000001800: Training Loss = 6.216e-01, Accuracy = 79.86%\n",
      "000001900: Training Loss = 5.968e-01, Accuracy = 80.50%\n",
      "000001950: #### 5 Epochs: Val Loss = 2.853e-01, Accuracy = 91.77%\n",
      "000002000: Training Loss = 5.639e-01, Accuracy = 81.61%\n",
      "000002100: Training Loss = 5.622e-01, Accuracy = 81.44%\n",
      "000002200: Training Loss = 5.383e-01, Accuracy = 82.88%\n",
      "000002300: Training Loss = 5.174e-01, Accuracy = 82.98%\n",
      "000002340: #### 6 Epochs: Val Loss = 2.600e-01, Accuracy = 92.71%\n",
      "000002400: Training Loss = 5.166e-01, Accuracy = 83.36%\n",
      "000002500: Training Loss = 5.201e-01, Accuracy = 83.06%\n",
      "000002600: Training Loss = 4.932e-01, Accuracy = 84.42%\n",
      "000002700: Training Loss = 4.624e-01, Accuracy = 85.34%\n",
      "000002730: #### 7 Epochs: Val Loss = 2.346e-01, Accuracy = 93.75%\n",
      "000002800: Training Loss = 4.575e-01, Accuracy = 85.11%\n",
      "000002900: Training Loss = 4.626e-01, Accuracy = 85.09%\n",
      "000003000: Training Loss = 4.690e-01, Accuracy = 85.08%\n",
      "000003100: Training Loss = 4.481e-01, Accuracy = 85.83%\n",
      "000003120: #### 8 Epochs: Val Loss = 2.204e-01, Accuracy = 93.44%\n",
      "000003200: Training Loss = 4.489e-01, Accuracy = 85.53%\n",
      "000003300: Training Loss = 4.519e-01, Accuracy = 85.55%\n",
      "000003400: Training Loss = 4.225e-01, Accuracy = 86.80%\n",
      "000003500: Training Loss = 4.239e-01, Accuracy = 86.38%\n",
      "000003510: #### 9 Epochs: Val Loss = 2.090e-01, Accuracy = 94.06%\n",
      "000003600: Training Loss = 4.197e-01, Accuracy = 86.83%\n",
      "000003700: Training Loss = 4.264e-01, Accuracy = 86.48%\n",
      "000003800: Training Loss = 4.145e-01, Accuracy = 86.83%\n",
      "000003900: Training Loss = 4.115e-01, Accuracy = 87.08%\n",
      "000003900: #### 10 Epochs: Val Loss = 1.919e-01, Accuracy = 94.38%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d6c92937dd838d4d5cad6b45cdc6deca670360475776cd4b2b4a79e3865f5ea"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}