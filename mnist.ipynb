{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "from os.path import normpath as fn\n",
    "import autograd \n",
    "\n",
    "np.random.seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load data\n",
    "data = np.load(fn('data/mnist_26k.npz'))\n",
    "\n",
    "train_im = np.float32(data['im_train'])/255.-0.5\n",
    "train_lb = data['lbl_train']\n",
    "\n",
    "val_im = np.float32(data['im_val'])/255.-0.5\n",
    "val_lb = data['lbl_val']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Inputs and parameters\n",
    "inp = autograd.Value()\n",
    "lab = autograd.Value()\n",
    "\n",
    "W1 = autograd.Param()\n",
    "B1 = autograd.Param()\n",
    "W2 = autograd.Param()\n",
    "B2 = autograd.Param()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Model\n",
    "y = autograd.matmul(inp,W1)\n",
    "y = autograd.add(y,B1)\n",
    "y = autograd.RELU(y)\n",
    "\n",
    "y = autograd.matmul(y,W2)\n",
    "y = autograd.add(y,B2) # This is our final prediction\n",
    "\n",
    "\n",
    "# Cross Entropy of Soft-max\n",
    "loss = autograd.smaxloss(y,lab)\n",
    "loss = autograd.mean(loss)\n",
    "\n",
    "# Accuracy\n",
    "acc = autograd.accuracy(y,lab)\n",
    "acc = autograd.mean(acc)\n",
    "\n",
    "###################################\n",
    "\n",
    "# Init Weights\n",
    "def xavier(shape):\n",
    "    sq = np.sqrt(3.0/np.prod(shape[:-1]))\n",
    "    return np.random.uniform(-sq,sq,shape)\n",
    "\n",
    "\n",
    "nHidden = 2048\n",
    "\n",
    "W1.set(xavier((28*28,nHidden)))\n",
    "B1.set(np.zeros((nHidden)))\n",
    "W2.set(xavier((nHidden,10)))\n",
    "B2.set(np.zeros((10)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Training loop\n",
    "BSZ=64\n",
    "lr=0.001\n",
    "\n",
    "NUM_EPOCH=50\n",
    "DISPITER=100\n",
    "batches = range(0,len(train_lb)-BSZ+1,BSZ)\n",
    "\n",
    "## Implement Momentum and uncomment following line\n",
    "autograd.init_momentum()\n",
    "\n",
    "\n",
    "niter=0; avg_loss = 0.; avg_acc = 0.\n",
    "for ep in range(NUM_EPOCH+1):\n",
    "\n",
    "    # As we train, let's keep track of val accuracy\n",
    "    vacc = 0.; vloss = 0.; viter = 0\n",
    "    for b in range(0,len(val_lb)-BSZ+1,BSZ):\n",
    "        inp.set(val_im[b:b+BSZ,:]); lab.set(val_lb[b:b+BSZ])\n",
    "        autograd.Forward()\n",
    "        viter = viter + 1;vacc = vacc + acc.top;vloss = vloss + loss.top\n",
    "    vloss = vloss / viter; vacc = vacc / viter * 100\n",
    "    print(\"%09d: #### %d Epochs: Val Loss = %.3e, Accuracy = %.2f%%\" % (niter,ep,vloss,vacc))\n",
    "    if ep == NUM_EPOCH:\n",
    "        break\n",
    "\n",
    "    # Shuffle Training Set\n",
    "    idx = np.random.permutation(len(train_lb))\n",
    "\n",
    "    # Train one epoch\n",
    "    for b in batches:\n",
    "        # Load a batch\n",
    "        inp.set(train_im[idx[b:b+BSZ],:])\n",
    "        lab.set(train_lb[idx[b:b+BSZ]])\n",
    "\n",
    "        autograd.Forward()\n",
    "        avg_loss = avg_loss + loss.top; avg_acc = avg_acc + acc.top;\n",
    "        niter = niter + 1\n",
    "        if niter % DISPITER == 0:\n",
    "            avg_loss = avg_loss / DISPITER; avg_acc = avg_acc / DISPITER * 100\n",
    "            print(\"%09d: Training Loss = %.3e, Accuracy = %.2f%%\" % (niter,avg_loss,avg_acc))\n",
    "            avg_loss = 0.; avg_acc = 0.;\n",
    "\n",
    "        autograd.Backward(loss)\n",
    "        autograd.momentum(lr,0.9)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000000000: #### 0 Epochs: Val Loss = 2.316e+00, Accuracy = 12.92%\n",
      "000000100: Training Loss = 1.769e+00, Accuracy = 55.66%\n",
      "000000200: Training Loss = 1.061e+00, Accuracy = 79.69%\n",
      "000000300: Training Loss = 7.699e-01, Accuracy = 84.88%\n",
      "000000390: #### 1 Epochs: Val Loss = 5.632e-01, Accuracy = 88.12%\n",
      "000000400: Training Loss = 6.561e-01, Accuracy = 84.41%\n",
      "000000500: Training Loss = 5.653e-01, Accuracy = 86.81%\n",
      "000000600: Training Loss = 5.331e-01, Accuracy = 86.77%\n",
      "000000700: Training Loss = 5.021e-01, Accuracy = 86.88%\n",
      "000000780: #### 2 Epochs: Val Loss = 4.119e-01, Accuracy = 90.42%\n",
      "000000800: Training Loss = 4.577e-01, Accuracy = 88.41%\n",
      "000000900: Training Loss = 4.418e-01, Accuracy = 88.48%\n",
      "000001000: Training Loss = 4.363e-01, Accuracy = 88.19%\n",
      "000001100: Training Loss = 4.189e-01, Accuracy = 89.00%\n",
      "000001170: #### 3 Epochs: Val Loss = 3.554e-01, Accuracy = 91.25%\n",
      "000001200: Training Loss = 3.952e-01, Accuracy = 89.39%\n",
      "000001300: Training Loss = 4.006e-01, Accuracy = 88.98%\n",
      "000001400: Training Loss = 3.933e-01, Accuracy = 89.19%\n",
      "000001500: Training Loss = 3.813e-01, Accuracy = 88.95%\n",
      "000001560: #### 4 Epochs: Val Loss = 3.260e-01, Accuracy = 91.67%\n",
      "000001600: Training Loss = 3.693e-01, Accuracy = 90.05%\n",
      "000001700: Training Loss = 3.604e-01, Accuracy = 89.69%\n",
      "000001800: Training Loss = 3.541e-01, Accuracy = 90.38%\n",
      "000001900: Training Loss = 3.554e-01, Accuracy = 90.02%\n",
      "000001950: #### 5 Epochs: Val Loss = 3.110e-01, Accuracy = 91.98%\n",
      "000002000: Training Loss = 3.454e-01, Accuracy = 90.55%\n",
      "000002100: Training Loss = 3.453e-01, Accuracy = 90.09%\n",
      "000002200: Training Loss = 3.369e-01, Accuracy = 90.30%\n",
      "000002300: Training Loss = 3.425e-01, Accuracy = 90.45%\n",
      "000002340: #### 6 Epochs: Val Loss = 2.992e-01, Accuracy = 92.19%\n",
      "000002400: Training Loss = 3.353e-01, Accuracy = 90.39%\n",
      "000002500: Training Loss = 3.326e-01, Accuracy = 90.83%\n",
      "000002600: Training Loss = 3.247e-01, Accuracy = 91.06%\n",
      "000002700: Training Loss = 3.210e-01, Accuracy = 90.73%\n",
      "000002730: #### 7 Epochs: Val Loss = 2.850e-01, Accuracy = 92.29%\n",
      "000002800: Training Loss = 3.121e-01, Accuracy = 91.00%\n",
      "000002900: Training Loss = 3.169e-01, Accuracy = 90.89%\n",
      "000003000: Training Loss = 3.093e-01, Accuracy = 91.33%\n",
      "000003100: Training Loss = 3.255e-01, Accuracy = 90.86%\n",
      "000003120: #### 8 Epochs: Val Loss = 2.751e-01, Accuracy = 92.29%\n",
      "000003200: Training Loss = 3.134e-01, Accuracy = 91.31%\n",
      "000003300: Training Loss = 3.093e-01, Accuracy = 91.42%\n",
      "000003400: Training Loss = 2.911e-01, Accuracy = 91.42%\n",
      "000003500: Training Loss = 3.033e-01, Accuracy = 91.48%\n",
      "000003510: #### 9 Epochs: Val Loss = 2.722e-01, Accuracy = 92.92%\n",
      "000003600: Training Loss = 3.013e-01, Accuracy = 91.98%\n",
      "000003700: Training Loss = 2.950e-01, Accuracy = 91.20%\n",
      "000003800: Training Loss = 2.955e-01, Accuracy = 91.34%\n",
      "000003900: Training Loss = 2.928e-01, Accuracy = 91.61%\n",
      "000003900: #### 10 Epochs: Val Loss = 2.638e-01, Accuracy = 92.71%\n",
      "000004000: Training Loss = 2.878e-01, Accuracy = 91.92%\n",
      "000004100: Training Loss = 2.911e-01, Accuracy = 91.80%\n",
      "000004200: Training Loss = 2.965e-01, Accuracy = 91.39%\n",
      "000004290: #### 11 Epochs: Val Loss = 2.613e-01, Accuracy = 92.92%\n",
      "000004300: Training Loss = 2.737e-01, Accuracy = 92.20%\n",
      "000004400: Training Loss = 2.659e-01, Accuracy = 92.31%\n",
      "000004500: Training Loss = 2.826e-01, Accuracy = 91.94%\n",
      "000004600: Training Loss = 2.866e-01, Accuracy = 91.73%\n",
      "000004680: #### 12 Epochs: Val Loss = 2.528e-01, Accuracy = 93.02%\n",
      "000004700: Training Loss = 2.852e-01, Accuracy = 91.78%\n",
      "000004800: Training Loss = 2.645e-01, Accuracy = 92.34%\n",
      "000004900: Training Loss = 2.815e-01, Accuracy = 91.81%\n",
      "000005000: Training Loss = 2.756e-01, Accuracy = 92.52%\n",
      "000005070: #### 13 Epochs: Val Loss = 2.503e-01, Accuracy = 93.23%\n",
      "000005100: Training Loss = 2.747e-01, Accuracy = 92.36%\n",
      "000005200: Training Loss = 2.608e-01, Accuracy = 92.62%\n",
      "000005300: Training Loss = 2.648e-01, Accuracy = 92.50%\n",
      "000005400: Training Loss = 2.652e-01, Accuracy = 92.09%\n",
      "000005460: #### 14 Epochs: Val Loss = 2.435e-01, Accuracy = 93.54%\n",
      "000005500: Training Loss = 2.836e-01, Accuracy = 91.77%\n",
      "000005600: Training Loss = 2.661e-01, Accuracy = 92.42%\n",
      "000005700: Training Loss = 2.590e-01, Accuracy = 92.58%\n",
      "000005800: Training Loss = 2.541e-01, Accuracy = 92.80%\n",
      "000005850: #### 15 Epochs: Val Loss = 2.458e-01, Accuracy = 93.75%\n",
      "000005900: Training Loss = 2.678e-01, Accuracy = 92.56%\n",
      "000006000: Training Loss = 2.469e-01, Accuracy = 93.17%\n",
      "000006100: Training Loss = 2.564e-01, Accuracy = 92.62%\n",
      "000006200: Training Loss = 2.501e-01, Accuracy = 92.72%\n",
      "000006240: #### 16 Epochs: Val Loss = 2.353e-01, Accuracy = 93.44%\n",
      "000006300: Training Loss = 2.527e-01, Accuracy = 93.22%\n",
      "000006400: Training Loss = 2.590e-01, Accuracy = 92.72%\n",
      "000006500: Training Loss = 2.475e-01, Accuracy = 93.14%\n",
      "000006600: Training Loss = 2.519e-01, Accuracy = 92.38%\n",
      "000006630: #### 17 Epochs: Val Loss = 2.318e-01, Accuracy = 93.75%\n",
      "000006700: Training Loss = 2.444e-01, Accuracy = 93.59%\n",
      "000006800: Training Loss = 2.427e-01, Accuracy = 92.98%\n",
      "000006900: Training Loss = 2.353e-01, Accuracy = 93.34%\n",
      "000007000: Training Loss = 2.593e-01, Accuracy = 92.45%\n",
      "000007020: #### 18 Epochs: Val Loss = 2.247e-01, Accuracy = 94.17%\n",
      "000007100: Training Loss = 2.369e-01, Accuracy = 93.42%\n",
      "000007200: Training Loss = 2.474e-01, Accuracy = 93.14%\n",
      "000007300: Training Loss = 2.484e-01, Accuracy = 92.91%\n",
      "000007400: Training Loss = 2.328e-01, Accuracy = 93.41%\n",
      "000007410: #### 19 Epochs: Val Loss = 2.242e-01, Accuracy = 94.38%\n",
      "000007500: Training Loss = 2.345e-01, Accuracy = 93.05%\n",
      "000007600: Training Loss = 2.334e-01, Accuracy = 93.50%\n",
      "000007700: Training Loss = 2.431e-01, Accuracy = 93.27%\n",
      "000007800: Training Loss = 2.333e-01, Accuracy = 93.45%\n",
      "000007800: #### 20 Epochs: Val Loss = 2.190e-01, Accuracy = 94.27%\n",
      "000007900: Training Loss = 2.183e-01, Accuracy = 93.75%\n",
      "000008000: Training Loss = 2.378e-01, Accuracy = 93.42%\n",
      "000008100: Training Loss = 2.366e-01, Accuracy = 93.19%\n",
      "000008190: #### 21 Epochs: Val Loss = 2.160e-01, Accuracy = 94.17%\n",
      "000008200: Training Loss = 2.307e-01, Accuracy = 93.59%\n",
      "000008300: Training Loss = 2.306e-01, Accuracy = 93.64%\n",
      "000008400: Training Loss = 2.304e-01, Accuracy = 93.33%\n",
      "000008500: Training Loss = 2.194e-01, Accuracy = 93.64%\n",
      "000008580: #### 22 Epochs: Val Loss = 2.130e-01, Accuracy = 94.48%\n",
      "000008600: Training Loss = 2.203e-01, Accuracy = 93.89%\n",
      "000008700: Training Loss = 2.164e-01, Accuracy = 94.08%\n",
      "000008800: Training Loss = 2.280e-01, Accuracy = 93.23%\n",
      "000008900: Training Loss = 2.222e-01, Accuracy = 93.73%\n",
      "000008970: #### 23 Epochs: Val Loss = 2.097e-01, Accuracy = 94.58%\n",
      "000009000: Training Loss = 2.233e-01, Accuracy = 94.02%\n",
      "000009100: Training Loss = 2.161e-01, Accuracy = 93.97%\n",
      "000009200: Training Loss = 2.123e-01, Accuracy = 93.94%\n",
      "000009300: Training Loss = 2.093e-01, Accuracy = 94.27%\n",
      "000009360: #### 24 Epochs: Val Loss = 2.104e-01, Accuracy = 94.69%\n",
      "000009400: Training Loss = 2.232e-01, Accuracy = 93.70%\n",
      "000009500: Training Loss = 2.236e-01, Accuracy = 93.67%\n",
      "000009600: Training Loss = 2.023e-01, Accuracy = 94.30%\n",
      "000009700: Training Loss = 2.159e-01, Accuracy = 94.05%\n",
      "000009750: #### 25 Epochs: Val Loss = 2.060e-01, Accuracy = 94.38%\n",
      "000009800: Training Loss = 2.066e-01, Accuracy = 93.83%\n",
      "000009900: Training Loss = 2.023e-01, Accuracy = 94.41%\n",
      "000010000: Training Loss = 2.148e-01, Accuracy = 93.88%\n",
      "000010100: Training Loss = 2.151e-01, Accuracy = 94.23%\n",
      "000010140: #### 26 Epochs: Val Loss = 2.032e-01, Accuracy = 94.79%\n",
      "000010200: Training Loss = 2.026e-01, Accuracy = 94.05%\n",
      "000010300: Training Loss = 2.028e-01, Accuracy = 94.41%\n",
      "000010400: Training Loss = 2.046e-01, Accuracy = 94.20%\n",
      "000010500: Training Loss = 2.038e-01, Accuracy = 94.48%\n",
      "000010530: #### 27 Epochs: Val Loss = 1.984e-01, Accuracy = 95.00%\n",
      "000010600: Training Loss = 2.055e-01, Accuracy = 94.64%\n",
      "000010700: Training Loss = 1.994e-01, Accuracy = 93.88%\n",
      "000010800: Training Loss = 1.911e-01, Accuracy = 94.72%\n",
      "000010900: Training Loss = 2.047e-01, Accuracy = 94.17%\n",
      "000010920: #### 28 Epochs: Val Loss = 1.955e-01, Accuracy = 94.48%\n",
      "000011000: Training Loss = 2.041e-01, Accuracy = 94.09%\n",
      "000011100: Training Loss = 2.047e-01, Accuracy = 94.14%\n",
      "000011200: Training Loss = 1.876e-01, Accuracy = 94.86%\n",
      "000011300: Training Loss = 1.905e-01, Accuracy = 94.72%\n",
      "000011310: #### 29 Epochs: Val Loss = 1.927e-01, Accuracy = 94.79%\n",
      "000011400: Training Loss = 1.791e-01, Accuracy = 94.89%\n",
      "000011500: Training Loss = 1.962e-01, Accuracy = 94.39%\n",
      "000011600: Training Loss = 2.008e-01, Accuracy = 94.23%\n",
      "000011700: Training Loss = 1.964e-01, Accuracy = 94.67%\n",
      "000011700: #### 30 Epochs: Val Loss = 1.922e-01, Accuracy = 94.79%\n",
      "000011800: Training Loss = 1.978e-01, Accuracy = 94.36%\n",
      "000011900: Training Loss = 1.805e-01, Accuracy = 94.86%\n",
      "000012000: Training Loss = 1.982e-01, Accuracy = 94.56%\n",
      "000012090: #### 31 Epochs: Val Loss = 1.893e-01, Accuracy = 95.21%\n",
      "000012100: Training Loss = 1.805e-01, Accuracy = 95.02%\n",
      "000012200: Training Loss = 1.850e-01, Accuracy = 94.84%\n",
      "000012300: Training Loss = 1.852e-01, Accuracy = 94.95%\n",
      "000012400: Training Loss = 1.852e-01, Accuracy = 94.77%\n",
      "000012480: #### 32 Epochs: Val Loss = 1.910e-01, Accuracy = 95.10%\n",
      "000012500: Training Loss = 1.879e-01, Accuracy = 94.72%\n",
      "000012600: Training Loss = 1.864e-01, Accuracy = 94.77%\n",
      "000012700: Training Loss = 1.816e-01, Accuracy = 94.78%\n",
      "000012800: Training Loss = 1.752e-01, Accuracy = 95.14%\n",
      "000012870: #### 33 Epochs: Val Loss = 1.842e-01, Accuracy = 95.00%\n",
      "000012900: Training Loss = 1.794e-01, Accuracy = 95.12%\n",
      "000013000: Training Loss = 1.816e-01, Accuracy = 95.05%\n",
      "000013100: Training Loss = 1.833e-01, Accuracy = 94.77%\n",
      "000013200: Training Loss = 1.712e-01, Accuracy = 95.38%\n",
      "000013260: #### 34 Epochs: Val Loss = 1.856e-01, Accuracy = 95.10%\n",
      "000013300: Training Loss = 1.867e-01, Accuracy = 94.62%\n",
      "000013400: Training Loss = 1.713e-01, Accuracy = 95.59%\n",
      "000013500: Training Loss = 1.805e-01, Accuracy = 94.98%\n",
      "000013600: Training Loss = 1.727e-01, Accuracy = 95.05%\n",
      "000013650: #### 35 Epochs: Val Loss = 1.777e-01, Accuracy = 95.31%\n",
      "000013700: Training Loss = 1.674e-01, Accuracy = 95.17%\n",
      "000013800: Training Loss = 1.707e-01, Accuracy = 95.34%\n",
      "000013900: Training Loss = 1.765e-01, Accuracy = 95.08%\n",
      "000014000: Training Loss = 1.649e-01, Accuracy = 95.45%\n",
      "000014040: #### 36 Epochs: Val Loss = 1.796e-01, Accuracy = 95.10%\n",
      "000014100: Training Loss = 1.671e-01, Accuracy = 95.58%\n",
      "000014200: Training Loss = 1.655e-01, Accuracy = 95.64%\n",
      "000014300: Training Loss = 1.645e-01, Accuracy = 95.48%\n",
      "000014400: Training Loss = 1.763e-01, Accuracy = 95.00%\n",
      "000014430: #### 37 Epochs: Val Loss = 1.742e-01, Accuracy = 95.52%\n",
      "000014500: Training Loss = 1.766e-01, Accuracy = 94.83%\n",
      "000014600: Training Loss = 1.639e-01, Accuracy = 95.70%\n",
      "000014700: Training Loss = 1.640e-01, Accuracy = 95.64%\n",
      "000014800: Training Loss = 1.659e-01, Accuracy = 95.30%\n",
      "000014820: #### 38 Epochs: Val Loss = 1.735e-01, Accuracy = 95.31%\n",
      "000014900: Training Loss = 1.527e-01, Accuracy = 95.94%\n",
      "000015000: Training Loss = 1.611e-01, Accuracy = 95.47%\n",
      "000015100: Training Loss = 1.660e-01, Accuracy = 95.41%\n",
      "000015200: Training Loss = 1.716e-01, Accuracy = 95.08%\n",
      "000015210: #### 39 Epochs: Val Loss = 1.748e-01, Accuracy = 95.42%\n",
      "000015300: Training Loss = 1.575e-01, Accuracy = 95.67%\n",
      "000015400: Training Loss = 1.464e-01, Accuracy = 95.95%\n",
      "000015500: Training Loss = 1.681e-01, Accuracy = 95.25%\n",
      "000015600: Training Loss = 1.677e-01, Accuracy = 95.50%\n",
      "000015600: #### 40 Epochs: Val Loss = 1.718e-01, Accuracy = 95.62%\n",
      "000015700: Training Loss = 1.629e-01, Accuracy = 95.45%\n",
      "000015800: Training Loss = 1.501e-01, Accuracy = 95.91%\n",
      "000015900: Training Loss = 1.593e-01, Accuracy = 95.69%\n",
      "000015990: #### 41 Epochs: Val Loss = 1.721e-01, Accuracy = 95.94%\n",
      "000016000: Training Loss = 1.543e-01, Accuracy = 95.78%\n",
      "000016100: Training Loss = 1.544e-01, Accuracy = 95.67%\n",
      "000016200: Training Loss = 1.476e-01, Accuracy = 96.14%\n",
      "000016300: Training Loss = 1.576e-01, Accuracy = 95.75%\n",
      "000016380: #### 42 Epochs: Val Loss = 1.705e-01, Accuracy = 95.62%\n",
      "000016400: Training Loss = 1.541e-01, Accuracy = 95.73%\n",
      "000016500: Training Loss = 1.531e-01, Accuracy = 96.05%\n",
      "000016600: Training Loss = 1.479e-01, Accuracy = 95.92%\n",
      "000016700: Training Loss = 1.541e-01, Accuracy = 95.72%\n",
      "000016770: #### 43 Epochs: Val Loss = 1.653e-01, Accuracy = 95.52%\n",
      "000016800: Training Loss = 1.465e-01, Accuracy = 95.64%\n",
      "000016900: Training Loss = 1.491e-01, Accuracy = 95.98%\n",
      "000017000: Training Loss = 1.444e-01, Accuracy = 96.08%\n",
      "000017100: Training Loss = 1.509e-01, Accuracy = 95.77%\n",
      "000017160: #### 44 Epochs: Val Loss = 1.640e-01, Accuracy = 95.52%\n",
      "000017200: Training Loss = 1.490e-01, Accuracy = 95.97%\n",
      "000017300: Training Loss = 1.580e-01, Accuracy = 95.58%\n",
      "000017400: Training Loss = 1.451e-01, Accuracy = 96.08%\n",
      "000017500: Training Loss = 1.423e-01, Accuracy = 96.09%\n",
      "000017550: #### 45 Epochs: Val Loss = 1.634e-01, Accuracy = 95.73%\n",
      "000017600: Training Loss = 1.362e-01, Accuracy = 96.42%\n",
      "000017700: Training Loss = 1.454e-01, Accuracy = 95.88%\n",
      "000017800: Training Loss = 1.406e-01, Accuracy = 96.44%\n",
      "000017900: Training Loss = 1.459e-01, Accuracy = 95.75%\n",
      "000017940: #### 46 Epochs: Val Loss = 1.582e-01, Accuracy = 95.62%\n",
      "000018000: Training Loss = 1.408e-01, Accuracy = 96.33%\n",
      "000018100: Training Loss = 1.415e-01, Accuracy = 96.05%\n",
      "000018200: Training Loss = 1.353e-01, Accuracy = 96.38%\n",
      "000018300: Training Loss = 1.439e-01, Accuracy = 96.11%\n",
      "000018330: #### 47 Epochs: Val Loss = 1.606e-01, Accuracy = 95.83%\n",
      "000018400: Training Loss = 1.449e-01, Accuracy = 95.88%\n",
      "000018500: Training Loss = 1.463e-01, Accuracy = 96.12%\n",
      "000018600: Training Loss = 1.387e-01, Accuracy = 96.38%\n",
      "000018700: Training Loss = 1.275e-01, Accuracy = 96.50%\n",
      "000018720: #### 48 Epochs: Val Loss = 1.607e-01, Accuracy = 95.94%\n",
      "000018800: Training Loss = 1.322e-01, Accuracy = 96.23%\n",
      "000018900: Training Loss = 1.334e-01, Accuracy = 96.30%\n",
      "000019000: Training Loss = 1.435e-01, Accuracy = 96.22%\n",
      "000019100: Training Loss = 1.352e-01, Accuracy = 96.34%\n",
      "000019110: #### 49 Epochs: Val Loss = 1.542e-01, Accuracy = 95.73%\n",
      "000019200: Training Loss = 1.395e-01, Accuracy = 96.22%\n",
      "000019300: Training Loss = 1.351e-01, Accuracy = 96.34%\n",
      "000019400: Training Loss = 1.335e-01, Accuracy = 96.38%\n",
      "000019500: Training Loss = 1.264e-01, Accuracy = 96.55%\n",
      "000019500: #### 50 Epochs: Val Loss = 1.530e-01, Accuracy = 95.94%\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "6d6c92937dd838d4d5cad6b45cdc6deca670360475776cd4b2b4a79e3865f5ea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}